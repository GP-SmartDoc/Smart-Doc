{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langgraph langchain_groq langchain_core chromadb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile Memory_Manager.py\nimport os\nimport time\nimport chromadb\nfrom chromadb.config import Settings\nfrom sentence_transformers import SentenceTransformer\n\n# ------------------------\n# Configuration\n# ------------------------\nCHROMA_PERSIST_DIR = \"chromadb_storage\"\nEMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n\n# ------------------------\n# Initialize Components\n# ------------------------\n# We initialize these outside the class so they are accessible when imported\nembedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n\nclient = chromadb.Client(\n    Settings(\n        persist_directory=CHROMA_PERSIST_DIR,\n        anonymized_telemetry=False\n    )\n)\n\ncollection_name = \"memory_collection\"\ncollection = client.get_or_create_collection(name=collection_name)\n\n# ------------------------\n# Memory Manager Class\n# ------------------------\nclass MemoryManager:\n    def __init__(self, collection, embedding_model):\n        self.collection = collection\n        self.embedding_model = embedding_model\n\n    def add_memory(self, text, id=None):\n        # Generate embedding\n        embedding = self.embedding_model.encode([text])[0].tolist()\n        \n        # CREATE METADATA AUTOMATICALLY\n        # This prevents the \"empty dict\" error\n        metadata = {\n            \"timestamp\": time.time(),\n            \"type\": \"user_message\",\n            \"content_preview\": text[:20] # Optional: store snippet in metadata\n        }\n        \n        # Generate ID if not provided\n        doc_id = id if id else os.urandom(8).hex()\n        \n        add_args = {\n            \"documents\": [text],\n            \"embeddings\": [embedding],\n            \"ids\": [doc_id],\n            \"metadatas\": [metadata], \n        }\n        \n        self.collection.add(**add_args)\n\n    def query_memory(self, query_text, n_results=3):\n        embedding = self.embedding_model.encode([query_text])[0].tolist()\n        results = self.collection.query(\n            query_embeddings=[embedding],\n            n_results=n_results\n        )\n        return results\n\n    def clear_memory(self):\n        self.collection.delete(where={})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile agent2.py\n# agent2.py\nfrom typing import TypedDict, List\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom langgraph.graph import StateGraph, START, END\nfrom langchain_groq import ChatGroq\n\n# import memory system (correct version lives here)\nfrom Memory_Manager import MemoryManager, collection, embedding_model\n\n# initialize memory manager\nmemory_manager = MemoryManager(collection, embedding_model)\n\n# Initialize LLM\nmodel = ChatGroq(\n    model=\"llama-3.1-8b-instant\",\n    api_key=\"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\"\n)\n\nclass AgentState(TypedDict):\n    messages: List[HumanMessage | AIMessage]\n    memory_context: str\n\ndef process(state: AgentState) -> AgentState:\n    user_msg = state[\"messages\"][-1].content\n\n    # query memory\n    results = memory_manager.query_memory(user_msg)\n    retrieved_docs = results.get(\"documents\", [[]])[0]\n    memory_text = \"\\n\".join(retrieved_docs) if retrieved_docs else \"No memory.\"\n\n    system_context = (\n        \"You have access to long-term memory.\\n\"\n        f\"Relevant memory:\\n{memory_text}\\n\"\n    )\n\n    msgs = [{\"role\": \"system\", \"content\": system_context}, *state[\"messages\"]]\n\n    response = model.invoke(msgs)\n    ai_msg = AIMessage(content=response.content)\n\n    # SAVE memory\n    memory_manager.add_memory(user_msg)\n\n    return {\n        \"messages\": state[\"messages\"] + [ai_msg],\n        \"memory_context\": memory_text\n    }\n\ngraph = StateGraph(AgentState)\ngraph.add_node(\"process\", process)\ngraph.add_edge(START, \"process\")\ngraph.add_edge(\"process\", END)\nagent = graph.compile()\n\ndef run_agent(user_text: str):\n    state = {\"messages\": [HumanMessage(content=user_text)], \"memory_context\": \"\"}\n    result = agent.invoke(state)\n    return result[\"messages\"][-1].content\n\ndef main():\n    print(\"Memory Agent is running...\")\n    print(\"Type 'exit' to quit.\\n\")\n\n    while True:\n        user_input = input(\"You: \")\n\n        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n            print(\"Agent: Goodbye!\")\n            break\n\n        response = run_agent(user_input)\n        print(\"Agent:\", response)\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from agent2 import run_agent\ndef main():\n    print(\"Memory Agent is running...\")\n    print(\"Type 'exit' to quit.\\n\")\n\n    while True:\n        user_input = input(\"You: \")\n\n        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n            print(\"Agent: Goodbye!\")\n            break\n\n        response = run_agent(user_input)\n        print(\"Agent:\", response)\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}